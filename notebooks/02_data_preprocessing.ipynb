{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8892b77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (303, 14)\n",
      "\n",
      "Missing values:\n",
      "ca      4\n",
      "thal    2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/heart_disease.csv')\n",
    "\n",
    "# Convert target to binary\n",
    "df['target'] = df['target'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()[df.isnull().sum() > 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34615b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before dropping missing values: 303\n",
      "Rows after dropping missing values: 297\n",
      "Dropped: 6 rows\n",
      "\n",
      "Missing values now:\n",
      "0\n",
      "\n",
      "Target distribution after cleaning:\n",
      "target\n",
      "0    160\n",
      "1    137\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# We have 4 missing in 'ca' and 2 in 'thal'\n",
    "# These are important features (high correlation with target)\n",
    "# Strategy: Since only 6 rows (2% of data), we'll drop them\n",
    "\n",
    "print(f\"Rows before dropping missing values: {len(df)}\")\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_clean = df.dropna()\n",
    "\n",
    "print(f\"Rows after dropping missing values: {len(df_clean)}\")\n",
    "print(f\"Dropped: {len(df) - len(df_clean)} rows\")\n",
    "print(f\"\\nMissing values now:\\n{df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# Check target distribution after dropping\n",
    "print(f\"\\nTarget distribution after cleaning:\")\n",
    "print(df_clean['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964154ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "age:\n",
      "  Lower bound: 28.50, Upper bound: 80.50\n",
      "  Number of outliers: 0\n",
      "\n",
      "trestbps:\n",
      "  Lower bound: 90.00, Upper bound: 170.00\n",
      "  Number of outliers: 9\n",
      "  Outlier values: [np.float64(172.0), np.float64(174.0), np.float64(178.0), np.float64(178.0), np.float64(180.0), np.float64(180.0), np.float64(180.0), np.float64(192.0), np.float64(200.0)]\n",
      "\n",
      "chol:\n",
      "  Lower bound: 113.50, Upper bound: 373.50\n",
      "  Number of outliers: 5\n",
      "  Outlier values: [np.float64(394.0), np.float64(407.0), np.float64(409.0), np.float64(417.0), np.float64(564.0)]\n",
      "\n",
      "thalach:\n",
      "  Lower bound: 83.50, Upper bound: 215.50\n",
      "  Number of outliers: 1\n",
      "  Outlier values: [np.float64(71.0)]\n",
      "\n",
      "oldpeak:\n",
      "  Lower bound: -2.40, Upper bound: 4.00\n",
      "  Number of outliers: 5\n",
      "  Outlier values: [np.float64(4.2), np.float64(4.2), np.float64(4.4), np.float64(5.6), np.float64(6.2)]\n"
     ]
    }
   ],
   "source": [
    "# Let's examine outliers more carefully\n",
    "# For medical data, extreme values might be real critical cases\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Check outliers in key numerical features\n",
    "numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "for col in numerical_cols:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df_clean, col)\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Lower bound: {lower:.2f}, Upper bound: {upper:.2f}\")\n",
    "    print(f\"  Number of outliers: {len(outliers)}\")\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"  Outlier values: {sorted(outliers[col].values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89b8e300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers capped successfully!\n",
      "\n",
      "Processed dataset shape: (297, 14)\n"
     ]
    }
   ],
   "source": [
    "# Medical reasoning for outliers:\n",
    "# - High cholesterol (>400) and high BP (>180) are real health conditions\n",
    "# - Very low max heart rate (<100) could indicate heart problems\n",
    "# - We'll keep outliers as they're medically valid\n",
    "\n",
    "# However, let's cap extreme values to reduce their impact\n",
    "def cap_outliers(data, column, lower_percentile=1, upper_percentile=99):\n",
    "    lower = data[column].quantile(lower_percentile/100)\n",
    "    upper = data[column].quantile(upper_percentile/100)\n",
    "    data[column] = data[column].clip(lower, upper)\n",
    "    return data\n",
    "\n",
    "# Make a copy before capping\n",
    "df_processed = df_clean.copy()\n",
    "\n",
    "# Cap extreme outliers (1st and 99th percentile)\n",
    "for col in ['trestbps', 'chol', 'oldpeak']:\n",
    "    df_processed = cap_outliers(df_processed, col, 1, 99)\n",
    "    \n",
    "print(\"Outliers capped successfully!\")\n",
    "print(f\"\\nProcessed dataset shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29816ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features created:\n",
      "  age_group chol_category bp_category\n",
      "0       >60    Borderline        High\n",
      "1       >60          High        High\n",
      "2       >60    Borderline      Normal\n",
      "3       <40          High    Elevated\n",
      "4     40-50    Borderline    Elevated\n",
      "5     50-60    Borderline      Normal\n",
      "6       >60          High    Elevated\n",
      "7     50-60          High      Normal\n",
      "8       >60          High    Elevated\n",
      "9     50-60    Borderline    Elevated\n"
     ]
    }
   ],
   "source": [
    "# Create some new features that might be useful\n",
    "\n",
    "# Age groups\n",
    "df_processed['age_group'] = pd.cut(df_processed['age'], \n",
    "                                    bins=[0, 40, 50, 60, 100], \n",
    "                                    labels=['<40', '40-50', '50-60', '>60'])\n",
    "\n",
    "# Cholesterol categories (based on medical standards)\n",
    "df_processed['chol_category'] = pd.cut(df_processed['chol'], \n",
    "                                        bins=[0, 200, 240, 1000], \n",
    "                                        labels=['Normal', 'Borderline', 'High'])\n",
    "\n",
    "# Blood pressure categories\n",
    "df_processed['bp_category'] = pd.cut(df_processed['trestbps'], \n",
    "                                      bins=[0, 120, 140, 1000], \n",
    "                                      labels=['Normal', 'Elevated', 'High'])\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(df_processed[['age_group', 'chol_category', 'bp_category']].head(10))\n",
    "\n",
    "# We'll keep these for EDA but won't use in initial model\n",
    "# (to keep it simple first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49937b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (297, 13)\n",
      "Target shape: (297,)\n",
      "\n",
      "Features: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
      "\n",
      "Training set: (237, 13)\n",
      "Test set: (60, 13)\n",
      "\n",
      "Target distribution in train set:\n",
      "target\n",
      "0    128\n",
      "1    109\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target distribution in test set:\n",
      "target\n",
      "0    32\n",
      "1    28\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df_clean.drop('target', axis=1)\n",
    "y = df_clean['target']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")\n",
    "\n",
    "# Split data: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTarget distribution in train set:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTarget distribution in test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f606792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature scaling completed!\n",
      "\n",
      "Before scaling (first row):\n",
      "age          54.0\n",
      "trestbps    124.0\n",
      "chol        266.0\n",
      "thalach     109.0\n",
      "oldpeak       2.2\n",
      "Name: 55, dtype: float64\n",
      "\n",
      "After scaling (first row):\n",
      "age        -0.085668\n",
      "trestbps   -0.462582\n",
      "chol        0.312737\n",
      "thalach    -1.827448\n",
      "oldpeak     0.967117\n",
      "Name: 55, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Scale numerical features for better model performance\n",
    "# Important: Fit scaler only on training data to avoid data leakage!\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Identify numerical columns to scale\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "\n",
    "# Transform test data (using same scaler)\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"\\nBefore scaling (first row):\")\n",
    "print(X_train[numerical_features].iloc[0])\n",
    "print(f\"\\nAfter scaling (first row):\")\n",
    "print(X_train_scaled[numerical_features].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a368cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models directory created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "print(\"Models directory created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30432b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All processed data saved!\n",
      "Ready for modeling!\n"
     ]
    }
   ],
   "source": [
    "# Save for modeling\n",
    "X_train_scaled.to_csv('../data/X_train.csv', index=False)\n",
    "X_test_scaled.to_csv('../data/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/y_test.csv', index=False)\n",
    "\n",
    "# Save the scaler for later use in deployment\n",
    "import pickle\n",
    "with open('../models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"✅ All processed data saved!\")\n",
    "print(\"Ready for modeling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542c4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
